{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Machine Learning \n",
    "# 48516 Isabel Mora Labarca\n",
    "\n",
    "This assignment will contain 3 questions with details as below. The due date is Feburary 27 (Sunday), 2022 23:59PM. Each late day will result in 20% loss of total points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1  (20 points) Make a plan before running your model\n",
    "\n",
    "Joana Gonzales is a young professional looking to diversify her investment portfolio. Joana graduated with a Masters in Business Analytics, and after four successful years as a product manager in a tech company, she has managed to save a sizable amount of money. She now wants to start diversifying her savings portfolio. So far, she has focused on traditional investments (stocks, bonds, etc.) and she now wants to look further afield. One asset class she is particularly interested in is peer-to-peer loans issued on online platforms. The high returns advertised by these platforms seem to be an attractive value proposition, and Joana is especially excited by the large amount of data these platforms make publicly available. With her data science background, she is hoping to apply machine learning tools to these data to come up with lucrative investment strategies. \n",
    "\n",
    "Peer-to-peer lending refers to the practice of lending money to individuals (or small businesses) via online services that match anonymous lenders with borrowers. Lenders can typically earn higher returns relative to savings and investment products offered by banking institutions. However, there is of course the risk that the borrower defaults on his or her loan. Interest rates are usually set by an intermediary platform on the basis of analyzing the borrower's credit (using features such as FICO score, employment status, annual income, debt-to-income ratio, number of open credit lines). The intermediary platform generates revenue by collecting a one-time fee on funded loans (from borrowers) and by charging a loan servicing fee to investors.\n",
    "\n",
    "The peer-to-peer lending industry in the United States started in February 2006 with the launch of Prosper, followed by LendingClub. In 2008, the Securities and Exchange Commission (SEC) required that peer-to-peer companies register their offerings as securities, pursuant to the Securities Act of 1933. Both Prosper and LendingClub gained approval from the SEC to offer investors notes backed by payments received on the loans. One of the interesting features of the peer-to-peer lending market is the richness of the historical data available. The two largest U.S. platforms (LendingClub and Prosper) have chosen to give free access to their data to potential investors. The definition of each loan status is as follows. Current refers to a loan that is still being reimbursed in a timely manner. Late corresponds to a loan on which a payment is between 16 and 120 days overdue. If the payment is delayed by more than 121 days, the loan is considered to be in Default. \n",
    "\n",
    "\n",
    "\n",
    "If you were Joana, your job is to define investment strategies. Given your knowledge of data science, below you need to write a plan (~300 words), using the steps specified from business problem to machine learning problem. Writing down in the below cell using Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type you answerr below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Joana’s Plan from Business Problem to Machine Learning Problem\n",
    "1.\t**Decide if machine learning is needed.** It would be hard to manually process each loan. Data is abundant, somewhat unstructured and humans would not be able to define patterns to set rules. Thus, task needs automation  ML is an option.\n",
    "2.\t**Define the business problem.** Since this particular asset class has higher risk to default, if I were Joana, I would start by predicting which ones will default (later focus on maximizing returns). Given loan data, predict which loan will default.\n",
    "3.\t**Data collection.** Thankfully, Joana has a rich meaningful dataset provided. Important is to verify data size and whether data is labeled (if not, only option is Unsupervised learning)\n",
    "4.\t**Find patterns** and common statistics from that dataset (mean, mode, quantiles, NaN values, frequency distributions through histograms). Verify data quality through identification of outliers and possible wrong values. Then, generate scatterplots of variables highly correlated is necessary. The idea is to identify if there are any very similar variables (to avoid multicollinearity in the model), as well as how they relate to the target variable. Lastly, apply techniques to deal with missing values such as replacing by mean or mode, or decide to drop the column if too many missing values. \n",
    "5.\tCheck whether **features are meaningful**. After the data curation, feature engineering would be the next step, i.e normalization, converting strings categories to dummy variables or converting continuous variables to categorical (OneHotEncoder, binning, StandardScaler...) \n",
    "6.\t**Evaluate success.** Idenitfy a success metric to evaluate model performance. If Joana wants to reduce the risk of default loans classified as non-default, weighted accuracy or precision would be some options (important to check for class imbalances).\n",
    "7.\t**Train model**: Divide data in train and test set. Select ~4 classification models. Target is \"Current\", \"Late\" and \"Default\", a multiclass classification problem, or a binary (by making Late and Default same class) is ideal. This depends on how risk averse is Joana. We could train Decision Trees, Random Forest, kNN and Softmax Regression and she could later use GridSearchCV to optimize hyperparameters for each model. Print confusion matrix for the models for evaluation, iterate training process to look for improvment. Select best model based on following: Accuracy, interpretability, fast, scalability.\n",
    "\n",
    "\n",
    "Advantages of classification models:\n",
    "- Feature importance and explainability in the model vs. clustering - hard to explain, needs business expertise, harder to maintain and evaluation is abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (50 points) Zestimate this house\n",
    "\n",
    "Purchasing a house is a very big decision for most of us. Companies such as Zillows collected tons of data regarding the listing and sold price of American houses and build the predictive model, named *Zestimate*. You are expected to build a model similar as Zestimate to predict house price in Boston. \n",
    "![zestimate](https://i0.wp.com/www.housesoldeasy.com/wp-content/uploads/Screen-Shot-2016-08-15-at-7.22.09-PM.png?resize=300%2C258&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = load_boston()\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 (10 points) \n",
    "Create train and test set, each contains 80% and 20% of the dataset, respectively, using *train_test_split* function in scikit-learn. Train a linear model on the train set and evaluate on the test set, report the training error and test error, respectively (as mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression intercept: 30.246750993923946\n",
      "Regression coefficients: [-1.13055924e-01  3.01104641e-02  4.03807204e-02  2.78443820e+00\n",
      " -1.72026334e+01  4.43883520e+00 -6.29636221e-03 -1.44786537e+00\n",
      "  2.62429736e-01 -1.06467863e-02 -9.15456240e-01  1.23513347e-02\n",
      " -5.08571424e-01]\n",
      "Training error (MSE): 21.641412753226312\n",
      "Test error (MSE): 24.291119474973456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(f\"Regression intercept: {lin_reg.intercept_}\") \n",
    "print(f\"Regression coefficients: {lin_reg.coef_}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_predict = lin_reg.predict(X_train)\n",
    "y_test_predict = lin_reg.predict(X_test)\n",
    "train_error = (mean_squared_error(y_train_predict, y_train))\n",
    "test_error = (mean_squared_error(y_test_predict, y_test))\n",
    "\n",
    "# Print results\n",
    "print(f\"Training error (MSE): {train_error}\")\n",
    "print(f\"Test error (MSE): {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 (10 points)\n",
    "\n",
    "Perform a 10-fold cross-validation on the whole data set. Show the averaged mean sqaured error on both train and test set at each fold, explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.002002    0.000975   -9.286947   -23.363228\n",
      "1  0.001000    0.000000  -14.151283   -22.882034\n",
      "2  0.000000    0.001000  -14.073606   -23.216075\n",
      "3  0.001026    0.000000  -35.206924   -20.771703\n",
      "4  0.001011    0.000000  -31.885117   -21.335426\n",
      "5  0.001001    0.000998  -19.835878   -22.363700\n",
      "6  0.001005    0.000000   -9.947269   -23.327221\n",
      "7  0.001000    0.000000 -168.375380   -11.959197\n",
      "8  0.001000    0.001000  -33.329745   -21.586295\n",
      "9  0.001000    0.001001  -10.960411   -23.189043\n",
      "The cross-validation average MSE for the train set is: 21.39939241710581\n",
      "The cross-validation average MSE for the test set is: 34.705255944524815\n",
      "The cross-validation average RMSE for the train set is: 4.625947731774086\n",
      "The cross-validation average RMSE for the test set is: 5.891116697581607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "cv_scores = cross_validate(lin_reg, X, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_df = pd.DataFrame(cv_scores) \n",
    "print(cv_scores_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error = (np.mean(cv_scores[\"train_score\"]))\n",
    "avg_test_error = (np.mean(cv_scores[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error}\")\n",
    "\n",
    "\n",
    "# We can look at Root Mean Squared Error (RMSE) for further analysis (not required by the question)\n",
    "print(f\"The cross-validation average RMSE for the train set is: {np.sqrt(-avg_train_error)}\")\n",
    "print(f\"The cross-validation average RMSE for the test set is: {np.sqrt(-avg_test_error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment: \n",
    "\n",
    "\"Scikit-Learn cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root.\" (p. 70, Géron 2017). As a result, I added a negative sign to the errors before being displayed.\n",
    "\n",
    "##### Findings:\n",
    "The test error is slightly higher than the training error, which is what is expected since the model was trained on the training data. Thus, the test error in some way represents how the model would behave with unseen data. When looking at the RMSE, we can see that the difference is not that high and there doesn't seem to be signs of overfitting. An interesting observation is that with cross-validation we have a higher test error than with splitting the data. This could be because when splitting the data, we randomly selected in the 80% of the training data more instances \"similar\" to those remaining in the test set -- \"the results can depend on a particular random choice for the pair of (train, validation) sets\". To overcome that problem, k-fold cross validation selects k folds of the whole data, trains the model on k-1 folds and takes as test the remaining fold. Thus, it is also important to look at each fold scores (it may happen that only one fold has a very good/bad score and affects the average of our results). For example, the fold in index #7 seems to have fitted more on the training set and returned a very high error on the test set (-168). This might explain why in this case our test error is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 (10 points) \n",
    " \n",
    "Add 2-degree squared polynomial features (with no interactions) and perform 10-fold cross-validation on the whole data set. Show the mean sqaured error on both train and test set at each fold, explain your findings.\n",
    "\n",
    "Hint: you may use sklearn.preprocessing.PolynomialFeatures and check how it produces the polynomial features with/without interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 26)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can manually create our polynomial features since we don't want interaction terms produced by sklearn.PolynomialFeatures\n",
    "import sys\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "X_2nd = np.hstack((X, X**2))  # our features + 2nd degree features (as clarified in Teams)\n",
    "X_2nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.001000    0.001005  -10.091861   -14.980271\n",
      "1  0.000997    0.000000   -8.791095   -14.969661\n",
      "2  0.000000    0.001000  -11.458221   -15.204169\n",
      "3  0.000998    0.000000  -22.518524   -13.665059\n",
      "4  0.000985    0.000000  -14.223065   -14.438738\n",
      "5  0.001000    0.001001   -8.780146   -14.940824\n",
      "6  0.001000    0.000000  -12.953433   -14.859624\n",
      "7  0.000000    0.000999 -104.037379    -7.419826\n",
      "8  0.001000    0.000000  -13.610872   -14.605032\n",
      "9  0.000000    0.001000  -50.771795   -14.050919\n",
      "The cross-validation average MSE for the train set is: 13.913412317032263\n",
      "The cross-validation average MSE for the test set is: 25.723639177011655\n",
      "The cross-validation average RMSE for the train set is: 3.7300686745731992\n",
      "The cross-validation average RMSE for the test set is: 5.071847708381203\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our model\n",
    "lin_reg_polydata = LinearRegression()\n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_2nd_data = cross_validate(lin_reg_polydata, X_2nd, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_2nd_df = pd.DataFrame(cv_scores_2nd_data) \n",
    "print(cv_scores_2nd_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_2data = (np.mean(cv_scores_2nd_data[\"train_score\"]))\n",
    "avg_test_error_2data = (np.mean(cv_scores_2nd_data[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_2data}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_2data}\")\n",
    "\n",
    "# Calculate RMSE (not required by the question, just by curiosity)\n",
    "print(f\"The cross-validation average RMSE for the train set is: {np.sqrt(-avg_train_error_2data)}\")\n",
    "print(f\"The cross-validation average RMSE for the test set is: {np.sqrt(-avg_test_error_2data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, as indicated by the question's hint, we could replicate this with with scikit-learn to test **with** interaction features (original features + interaction features + 2nd degree features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.005002    0.000000  -36.796836    -5.542527\n",
      "1  0.002001    0.000000   -9.203243    -6.096625\n",
      "2  0.003014    0.000000  -13.143433    -6.164152\n",
      "3  0.004003    0.000000  -34.972319    -5.171784\n",
      "4  0.003010    0.000988 -296.418468    -5.948657\n",
      "5  0.003999    0.000000  -26.469038    -6.135376\n",
      "6  0.002997    0.001000  -24.557012    -5.963459\n",
      "7  0.004034    0.000000 -310.038873    -3.673894\n",
      "8  0.001000    0.000998 -110.669118    -4.967707\n",
      "9  0.003002    0.000000  -78.187051    -5.721595\n",
      "The cross-validation average MSE for the train set is: 5.538577770311853\n",
      "The cross-validation average MSE for the test set is: 94.04553914969702\n",
      "The cross-validation average RMSE for the train set is: 2.35341831604835\n",
      "The cross-validation average RMSE for the test set is: 9.697707932789944\n"
     ]
    }
   ],
   "source": [
    "# Create poly features with interaction terms \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False) \n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Instantiate our model\n",
    "lin_reg_polydata = LinearRegression()\n",
    "\n",
    "# Perform cross-validation (the default sklearn polyfeatures includes interaction terms)\n",
    "cv_scores_polydata = cross_validate(lin_reg_polydata, X_poly, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_poly_df = pd.DataFrame(cv_scores_polydata) \n",
    "print(cv_scores_poly_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_polydata = (np.mean(cv_scores_polydata[\"train_score\"]))\n",
    "avg_test_error_polydata = (np.mean(cv_scores_polydata[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_polydata}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_polydata}\")\n",
    "\n",
    "# Calculate RMSE (not required by the question, just by curiosity)\n",
    "print(f\"The cross-validation average RMSE for the train set is: {np.sqrt(-avg_train_error_polydata)}\")\n",
    "print(f\"The cross-validation average RMSE for the test set is: {np.sqrt(-avg_test_error_polydata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Findings and comments: \n",
    "\n",
    "Regarding the model with 2nd degree transformed data with no interactions, we see that the MSE for the train and testing set are lower than the model with no polynomial features. By transforming our features to a 2nd degree, we were able to better capture the patterns of the data. It is always a good idea to try several degrees before choosing the model. However, it seems this one is performing better than the linear regression with linear features only.\n",
    "\n",
    "Regarding the model with interaction features, since we are using 13 features, including the interaction terms results in 104 terms in total {Original : 13, 2nd degree of each : 13, Interaction terms: 78 (13*12)/2)}. This makes the model with interaction terms a worse option since the train error remains low but the test error is very high. This is a common feature of overfitting, which means that there is a high variance and low bias in our model. This makes our model hard to generalize (to apply to unseen data). This is probably a result of too much noise from the interaction terms in the model. I still tested the PolynomialFeatures option both with bias True and False, and by setting include_bias = False we actually reduced the test error from 122 to 94). As I mentioned earlier with the folds, some folds in this model have more or less around 30 as the error, but given that more than a couple are way higher the error is also affected.\n",
    "\n",
    "Lastly, it is generally a good practice to standardize our features. In the code above I tested both with and without standardization, but because it is only a 2nd degree polynomial adding the standardization didn't change our model performance(it usually makes more sense with higher degree polynomials because higher the degree is, higher the \"ratio\" between the original features and the polynomial ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.4 (20 points)\n",
    "\n",
    "Perform cross-validation using ridge regression and lasso regression on different feature combinations (linear features vs. polynomial features obtained earlier respectively. Explain which method works better in this case. Check the coefficients and explain the differences between ridge regression and lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Perform cross-validation using ridge regression  (L2-norm penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Ridge regression on linear features:\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.002972    0.000000   -9.205960   -23.364503\n",
      "1  0.000000    0.000000  -14.020688   -22.883501\n",
      "2  0.001000    0.000000  -13.886969   -23.217438\n",
      "3  0.001000    0.000000  -35.304928   -20.772916\n",
      "4  0.000998    0.000000  -31.752108   -21.336776\n",
      "5  0.001001    0.001000  -19.817792   -22.365140\n",
      "6  0.000999    0.001001   -9.816963   -23.328829\n",
      "7  0.000999    0.000000 -167.718605   -11.959861\n",
      "8  0.000998    0.000000  -33.235367   -21.587505\n",
      "9  0.000000    0.000000  -10.777502   -23.190893\n",
      "The cross-validation average MSE for the train set is: 21.400736109995773\n",
      "The cross-validation average MSE for the test set is: 34.553688206729625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ridge on linear features\n",
    "# Instantiate Ridge regression\n",
    "ridge_reg = Ridge()\n",
    "\n",
    "# Scale data (essential for L1 and L2 regularization)\n",
    "X_scaled = StandardScaler().fit_transform(X) \n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_linear_ridge = cross_validate(ridge_reg, X_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_linear_ridge_df = pd.DataFrame(cv_scores_linear_ridge) \n",
    "print(\"Cross-validation scores using Ridge regression on linear features:\")\n",
    "print(cv_scores_linear_ridge_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_ridge_linear = (np.mean(cv_scores_linear_ridge[\"train_score\"]))\n",
    "avg_test_error_ridge_linear = (np.mean(cv_scores_linear_ridge[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_ridge_linear}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_ridge_linear}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Ridge regression on polynomial features (no interaction terms):\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.000995    0.001000   -9.620575   -15.292400\n",
      "1  0.001004    0.000000   -9.690075   -15.247261\n",
      "2  0.000000    0.001017   -7.799589   -15.532068\n",
      "3  0.000986    0.001003  -22.673914   -14.019436\n",
      "4  0.000998    0.000000  -15.459166   -14.730196\n",
      "5  0.001001    0.001000  -11.146665   -15.238333\n",
      "6  0.000998    0.000000   -9.849589   -15.241393\n",
      "7  0.000998    0.000000 -114.428832    -7.885734\n",
      "8  0.001001    0.000000  -15.468322   -14.926206\n",
      "9  0.000000    0.001001  -15.753151   -14.634941\n",
      "The cross-validation average MSE for the train set is: 14.274796664916385\n",
      "The cross-validation average MSE for the test set is: 23.188987818851977\n"
     ]
    }
   ],
   "source": [
    "# Ridge on polynomial with no interaction\n",
    "# Scale data (essential for L1 and L2 regularization)\n",
    "X_2nd_scaled = StandardScaler().fit_transform(X_2nd) \n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_2nd_ridge = cross_validate(ridge_reg, X_2nd_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_2nd_ridge_df = pd.DataFrame(cv_scores_2nd_ridge) \n",
    "\n",
    "print(\"Cross-validation scores using Ridge regression on polynomial features (no interaction terms):\")\n",
    "print(cv_scores_2nd_ridge_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_ridge_2nd = (np.mean(cv_scores_2nd_ridge[\"train_score\"]))\n",
    "avg_test_error_ridge_2nd = (np.mean(cv_scores_2nd_ridge[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_ridge_2nd}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_ridge_2nd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Ridge regression on polynomial features (with interaction terms):\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.001001    0.000000  -23.465139    -7.007333\n",
      "1  0.002004    0.000000   -5.409577    -7.946064\n",
      "2  0.001001    0.000000  -13.835534    -7.972552\n",
      "3  0.002000    0.000000  -26.635341    -6.580534\n",
      "4  0.001999    0.000000  -11.164627    -7.638689\n",
      "5  0.001001    0.000999   -8.275713    -7.863941\n",
      "6  0.000999    0.000000   -7.042945    -7.837769\n",
      "7  0.000999    0.000000 -124.234489    -4.990659\n",
      "8  0.001001    0.000000  -26.746230    -6.998509\n",
      "9  0.002014    0.000000  -16.149869    -7.347603\n",
      "The cross-validation average MSE for the train set is: 7.218365407466683\n",
      "The cross-validation average MSE for the test set is: 26.295946425772236\n"
     ]
    }
   ],
   "source": [
    "# Ridge on polynomial with interaction\n",
    "# Scale data (essential for L1 and L2 regularization)\n",
    "X_poly_scaled = StandardScaler().fit_transform(X_poly) \n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_poly_ridge = cross_validate(ridge_reg, X_poly_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_poly_ridge_df = pd.DataFrame(cv_scores_poly_ridge) \n",
    "\n",
    "print(\"Cross-validation scores using Ridge regression on polynomial features (with interaction terms):\")\n",
    "print(cv_scores_poly_ridge_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_ridge_poly = (np.mean(cv_scores_poly_ridge[\"train_score\"]))\n",
    "avg_test_error_ridge_poly = (np.mean(cv_scores_poly_ridge[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_ridge_poly}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_ridge_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Perform cross-validation using lasso regression (L1-norm penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Lasso regularization (L1) on linear features:\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.001001    0.000000   -8.568525   -30.242354\n",
      "1  0.002000    0.000000   -9.392887   -30.141388\n",
      "2  0.000000    0.000000   -7.527419   -30.095249\n",
      "3  0.000995    0.000000  -58.182680   -27.063448\n",
      "4  0.000000    0.000000  -32.689138   -28.614591\n",
      "5  0.001001    0.000000  -44.189401   -29.196016\n",
      "6  0.000000    0.001012  -13.957288   -29.669342\n",
      "7  0.000986    0.000000 -147.236792   -16.539227\n",
      "8  0.000997    0.000000  -47.110071   -27.822716\n",
      "9  0.001001    0.000000   -9.799905   -30.121426\n",
      "The cross-validation average MSE for the train set is: 27.950575674244448\n",
      "The cross-validation average MSE for the test set is: 37.865410649737534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso on linear features\n",
    "# Instantiate Lasso regression\n",
    "lasso_reg = Lasso(max_iter = 5000)\n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_linear_lasso = cross_validate(lasso_reg, X_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_linear_lasso_df = pd.DataFrame(cv_scores_linear_lasso) \n",
    "print(\"Cross-validation scores using Lasso regularization (L1) on linear features:\")\n",
    "print(cv_scores_linear_lasso_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_lasso_linear = (np.mean(cv_scores_linear_lasso[\"train_score\"]))\n",
    "avg_test_error_lasso_linear = (np.mean(cv_scores_linear_lasso[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_lasso_linear}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_lasso_linear}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Lasso regularization (L1) on polynomial features (no interaction terms):\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.001001    0.001015   -8.057489   -28.458445\n",
      "1  0.000986    0.000000   -7.721153   -28.480050\n",
      "2  0.001006    0.000000   -6.949513   -28.345592\n",
      "3  0.001001    0.001018  -53.245763   -25.697721\n",
      "4  0.001989    0.000987  -27.272405   -27.215473\n",
      "5  0.000999    0.000000  -38.431906   -27.745849\n",
      "6  0.001000    0.000000  -12.229015   -27.965299\n",
      "7  0.001001    0.000999 -142.162076   -15.012446\n",
      "8  0.001002    0.000000  -47.357895   -26.060651\n",
      "9  0.001022    0.001103   -9.857724   -28.271055\n",
      "The cross-validation average MSE for the train set is: 26.32525818606435\n",
      "The cross-validation average MSE for the test set is: 35.328493820482564\n"
     ]
    }
   ],
   "source": [
    "# Lasso on polynomial with no interaction terms\n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_2nd_lasso = cross_validate(lasso_reg, X_2nd_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_2nd_lasso_df = pd.DataFrame(cv_scores_2nd_lasso) \n",
    "\n",
    "print(\"Cross-validation scores using Lasso regularization (L1) on polynomial features (no interaction terms):\")\n",
    "print(cv_scores_2nd_lasso_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_lasso_2nd = (np.mean(cv_scores_2nd_lasso[\"train_score\"]))\n",
    "avg_test_error_lasso_2nd = (np.mean(cv_scores_2nd_lasso[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_lasso_2nd}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_lasso_2nd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores using Lasso regression on polynomial features (with interaction terms):\n",
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.002994    0.000000   -7.906729   -24.505818\n",
      "1  0.002001    0.000000   -7.514007   -24.624585\n",
      "2  0.000999    0.001001   -6.029802   -24.496837\n",
      "3  0.002004    0.000000  -47.371793   -22.706793\n",
      "4  0.001977    0.000000  -23.622911   -23.656898\n",
      "5  0.001991    0.000999  -37.132872   -24.408276\n",
      "6  0.001997    0.000000  -14.577696   -24.187641\n",
      "7  0.010010    0.001007 -136.291900   -13.847292\n",
      "8  0.000985    0.001001  -35.405780   -23.767637\n",
      "9  0.000997    0.001004  -10.221469   -24.063332\n",
      "The cross-validation average MSE for the train set is: 23.026510975779882\n",
      "The cross-validation average MSE for the test set is: 32.607495922352065\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression on polynomial features (with interaction terms)\n",
    "\n",
    "# Perform cross-validation \n",
    "cv_scores_poly_lasso = cross_validate(lasso_reg, X_poly_scaled, y, scoring=\"neg_mean_squared_error\", cv=10, return_train_score = True)\n",
    "\n",
    "# Display MSE on train and test set at each fold\n",
    "cv_scores_poly_lasso_df = pd.DataFrame(cv_scores_poly_lasso) \n",
    "print(\"Cross-validation scores using Lasso regression on polynomial features (with interaction terms):\")\n",
    "print(cv_scores_poly_lasso_df)\n",
    "\n",
    "# Calculate averaged MSE for both train and test sets\n",
    "avg_train_error_lasso_poly = (np.mean(cv_scores_poly_lasso[\"train_score\"]))\n",
    "avg_test_error_lasso_poly = (np.mean(cv_scores_poly_lasso[\"test_score\"]))\n",
    "                  \n",
    "print(f\"The cross-validation average MSE for the train set is: {-avg_train_error_lasso_poly}\")\n",
    "print(f\"The cross-validation average MSE for the test set is: {-avg_test_error_lasso_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE:\n",
      "Ridge regression:\n",
      "\tRidge Linear: 21.4\n",
      "\tRidge 2nd degree: 14.27\n",
      "\tRidge 2nd degree with interaction terms: 7.22\n",
      "\n",
      "Lasso regression:\n",
      "\tLasso Linear: 27.95\n",
      "\tLasso 2nd degree: 26.33\n",
      "\tLasso 2nd degree with interaction terms: 23.03\n",
      "\n",
      "Test set MSE:\n",
      "\n",
      "Ridge regression:\n",
      "\tRidge Linear: 34.55\n",
      "\tRidge 2nd degree: 23.19\n",
      "\tRidge 2nd degree with interaction terms: 26.3\n",
      "\n",
      "Lasso regression:\n",
      "\tLasso Linear: 37.87\n",
      "\tLasso 2nd degree: 35.33\n",
      "\tLasso 2nd degree with interaction terms: 32.61\n"
     ]
    }
   ],
   "source": [
    "print('Training set MSE:')\n",
    "print(\"Ridge regression:\")\n",
    "print(f\"\\tRidge Linear: {round(-avg_train_error_ridge_linear, 2)}\")\n",
    "print(f\"\\tRidge 2nd degree: {round(-avg_train_error_ridge_2nd, 2)}\")\n",
    "print(f\"\\tRidge 2nd degree with interaction terms: {round(-avg_train_error_ridge_poly, 2)}\")\n",
    "\n",
    "print(\"\\nLasso regression:\")\n",
    "print(f\"\\tLasso Linear: {round(-avg_train_error_lasso_linear, 2)}\")\n",
    "print(f\"\\tLasso 2nd degree: {round(-avg_train_error_lasso_2nd, 2)}\")\n",
    "print(f\"\\tLasso 2nd degree with interaction terms: {round(-avg_train_error_lasso_poly, 2)}\")\n",
    "\n",
    "print('\\nTest set MSE:')\n",
    "print(\"\\nRidge regression:\")\n",
    "print(f\"\\tRidge Linear: {round(-avg_test_error_ridge_linear, 2)}\")\n",
    "print(f\"\\tRidge 2nd degree: {round(-avg_test_error_ridge_2nd, 2)}\")\n",
    "print(f\"\\tRidge 2nd degree with interaction terms: {round(-avg_test_error_ridge_poly, 2)}\")\n",
    "\n",
    "print(\"\\nLasso regression:\")\n",
    "print(f\"\\tLasso Linear: {round(-avg_test_error_lasso_linear, 2)}\")\n",
    "print(f\"\\tLasso 2nd degree: {round(-avg_test_error_lasso_2nd, 2)}\")\n",
    "print(f\"\\tLasso 2nd degree with interaction terms: {round(-avg_test_error_lasso_poly, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check coefficients for Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for Ridge regression with linear features:\n",
      "[-0.99218679  0.6777488   0.2522143   0.72248078 -1.99083465  3.15157218\n",
      " -0.17726162 -3.04502895  2.17324941 -1.69555879 -2.02783351  1.127197\n",
      " -3.59897667]\n",
      "\n",
      "Coefficients for Ridge regression with polynomial features (2nd degree):\n",
      "[-2.87651226 -1.03002245 -1.44282184  0.33495363 -0.36945219 -9.37990822\n",
      " -0.59718787 -5.05505012  3.91586128 -2.57957988 -4.80867551  2.21546706\n",
      " -9.46455982  1.2825661   1.17436441  1.51086154  0.33495363 -1.88668172\n",
      " 11.78564121  0.67848928  2.52731219 -0.66812359  0.44046655  3.02607263\n",
      " -1.67226541  5.46745655]\n",
      "\n",
      "Coefficients for Ridge regression with polynomial features (2nd degree) + interaction terms:\n",
      "[ 0.27603437 -2.36714151  0.336439    2.41994869  2.1682028   5.22273689\n",
      "  4.01163896 -4.88468031  4.98553945  2.20607581  0.94022158  2.92468745\n",
      "  1.682618    1.32679955  0.12114909  1.09117818  2.53357283 -2.38225571\n",
      "  0.48561339 -1.30664898 -0.79257428 -1.03622182 -0.13732942  0.01345575\n",
      " -0.46413183  1.34755698  0.74253784 -0.19351136 -0.14990743 -0.63038957\n",
      "  1.43014436 -0.05444113 -1.40400923 -0.33579284  2.20091849  1.66575208\n",
      " -0.59148289 -0.82408251  1.84498231  0.13413794  1.32705266 -1.22701284\n",
      "  1.97435442 -0.2523557   1.90498733  1.96204219 -2.5824085   0.53008509\n",
      " -3.89302791  2.41994869 -2.95980544 -5.19804049  1.05778229  1.27365472\n",
      " -1.44575223 -0.31847663  0.4021954   1.56890787 -0.97230367 -0.58168301\n",
      " -0.87852536 -2.00749565 -0.05034698 -1.51898667 -1.35963742 -2.21815768\n",
      "  0.05877471  2.04250596  6.15008053 -2.38114719  0.34806    -3.51212656\n",
      " -5.73288239 -4.79162034  0.85360574 -3.72906103  0.19560337  0.39151009\n",
      "  3.29694416 -0.5528087   0.61767982 -3.21000431 -2.66458158  4.53977061\n",
      "  0.01505567 -1.31081854 -0.58837996 -1.62578604  1.26394303 -2.87331997\n",
      "  1.2227235   4.07255453 -0.42721357 -3.70423025 -0.65047543  4.68262428\n",
      " -0.12634826 -2.92921311  1.58162886  1.1177106   0.7545883  -0.71663099\n",
      " -1.38199578  4.67679982]\n"
     ]
    }
   ],
   "source": [
    "# Train Ridge regression with train set and check coefficients for linear features\n",
    "# Our X train is not scaled yet\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train) \n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"Coefficients for Ridge regression with linear features:\")\n",
    "print(ridge_reg.coef_)\n",
    "\n",
    "# I will check as well the coefficients for polynomial and polynomial with interactions model\n",
    "# However, for simplicity, I will not split them in train and test since we are more interested in coefficients than in metrics\n",
    "\n",
    "print(f\"\\nCoefficients for Ridge regression with polynomial features (2nd degree):\")\n",
    "ridge_reg.fit(X_2nd_scaled, y)\n",
    "print(ridge_reg.coef_)\n",
    "\n",
    "print(f\"\\nCoefficients for Ridge regression with polynomial features (2nd degree) + interaction terms:\")\n",
    "ridge_reg.fit(X_poly_scaled, y)\n",
    "print(ridge_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check coefficients for Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for Lasso regression with linear features:\n",
      "[-0.01238985  0.         -0.          0.0384036  -0.          3.07945798\n",
      " -0.         -0.         -0.         -0.         -1.22090499  0.44979801\n",
      " -3.37659049]\n",
      "\n",
      "Coefficients for Lasso regression with polynomial features (2nd degree):\n",
      "[-0.          0.         -0.          0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -1.23251716  0.21925006\n",
      " -3.3807687  -0.          0.         -0.          0.         -0.\n",
      "  3.09096166 -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.        ]\n",
      "\n",
      "Coefficients for Lasso regression with polynomial features (2nd degree) + interaction terms:\n",
      "[-0.          0.         -0.          0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.99073601  0.\n",
      " -0.         -0.          0.         -0.          0.06751978 -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.          0.         -0.         -0.\n",
      " -0.         -0.05012457 -0.         -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  0.         -0.          3.30027759 -0.         -0.         -0.\n",
      " -0.         -0.          0.42003344 -3.49770408 -0.         -0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      " -0.         -0.14600957 -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.          0.         -0.          0.\n",
      " -0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Train Lasso regression with train set and check coefficients for linear features\n",
    "# Our X train is not scaled yet\n",
    "\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train) \n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"Coefficients for Lasso regression with linear features:\")\n",
    "print(lasso_reg.coef_)\n",
    "\n",
    "# I will check as well the coefficients for polynomial and polynomial with interactions model\n",
    "# However, for simplicity, I will not split them in train and test since we are more interested in coefficients than in metrics\n",
    "\n",
    "print(f\"\\nCoefficients for Lasso regression with polynomial features (2nd degree):\")\n",
    "lasso_reg.fit(X_2nd_scaled, y)\n",
    "print(lasso_reg.coef_)\n",
    "\n",
    "print(f\"\\nCoefficients for Lasso regression with polynomial features (2nd degree) + interaction terms:\")\n",
    "lasso_reg.fit(X_poly_scaled, y)\n",
    "print(lasso_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "\n",
    "- Looking at the MSE training errors, the errors are a little lower for the Ridge regression than for Lasso. This is due to the fact that Lasso completly ommits some variables that doesn't deem important by setting their coefficient at 0. Thus, the training error may not fit as well as usual.\n",
    "- When looking at the MSE test errors, we also see that Ridge regression has lower scores than the Lasso regression, but it is more noticeable in the polynomial regressions (in the linear regressions test errors Ridge has MSE of ~34 vs Lasso ~37)\n",
    "- Overall, the 2nd degree regression seems to generalize better than the linear features model, as we have seen before. \n",
    "- It is interesting that in the Ridge regression, the test MSE is higher on the 2nd degree polynomial features **with** interactions than the MSE on no interaction. On the contrary, Lasso has a lower test MSE of the polynomial with interactions than in the ones with no interaction. This might be due to the fact that, as we discussed earlier, the polynomial with interaction terms might have a lot of noise and as such, Lasso was succesfully able to eliminate the weights of the irrelevant variables from the regression (i.e., set them to zero).\n",
    "- As we see when checking the coefficients, there are only between 4-5 variables that Lasso considers as relevant for the model, while all the other ones have coefficients at 0.\n",
    "\n",
    "##### Further comments:\n",
    "- Both Lasso and Ridge regression were used with the default alphas for the purpose of the exercise, but we could implement a Grid Search to optimize the alpha that returns the best model in each case. \n",
    "- Overall, we mostly always want regularization, and in this case we were able to see that it helped decrease the MSE.\n",
    "- Compared to the regressions without regularization, the MSE of the regularized models were a bit better (lower) in most of the cases, and severely lower than the polynomial regression with interactions (non-regularized).\n",
    "- The best model depends on how simple we want our model to be. If we want to keep polynomial features **with** interaction terms, then Lasso would be a good option since it eliminates the noise. As we saw that 2nd degree performs better than linear features, I would stay with 2nd degree features. Now, if we want to use the 2nd degree data **without** interaction terms, Ridge has the lowest MSE in the test set, which will make Ridge a good option.\n",
    "- However, it is important to also have a simple and interpretable model, for which we could sacrifice a little bit of error in exchange of explainability and simplicity. In this case, we don't know if only few variables are relevant to the target, but further analysis would be required to precisely determine how many are relavant for our target and if there are only a few, Lasso would be best.\n",
    "- Moreover, \"Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated\" (Géron, 2017). And, it randomly drops one from the highly correlated ones. This is not a desirable feature because we might prefer one variable that makes more sense in terms of explaining our model. For this issue, elastic net would be a good option or we could simply remain with Ridge. \n",
    "- I decided to scale before fitting the data in both Lasso and Ridge regression because they \"assume that features are centered around zero and have variance in the same order\". (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "- Just to check, I calculated the correlation between the features and the variable below, and it seems that only 3 variables have a correlation higher than .5 with the target variable. It would be interesting to see which variables Lasso considered as relevant, but they are probably some of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house = pd.DataFrame(dataset.data, columns =  dataset.feature_names)\n",
    "df_house_target = pd.DataFrame(dataset.target).rename(columns={0: \"MEDV\"})\n",
    "list_concat = (df_house, df_house_target)\n",
    "df_all = pd.concat(list_concat, axis = 1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV       1.000000\n",
       "RM         0.695360\n",
       "ZN         0.360445\n",
       "B          0.333461\n",
       "DIS        0.249929\n",
       "CHAS       0.175260\n",
       "AGE       -0.376955\n",
       "RAD       -0.381626\n",
       "CRIM      -0.388305\n",
       "NOX       -0.427321\n",
       "TAX       -0.468536\n",
       "INDUS     -0.483725\n",
       "PTRATIO   -0.507787\n",
       "LSTAT     -0.737663\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrmat = df_all.corr()\n",
    "pearson = corrmat['MEDV']\n",
    "pearson.sort_values(axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (30 points) Cancer detection\n",
    "\n",
    "Given a dataset with features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, which describes characteristics of the cell nuclei present in the image, let's try to predict whether the patients are diagnosed as Malignant (M) or Benign (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "X, y = dataset[\"data\"], dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.1 (10 points) \n",
    "Use logistic regression to train the dataset through cross-validation, report the score on train and test set, respectively. Explain what do you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter = 5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation \n",
    "X_scaled = StandardScaler().fit_transform(X) \n",
    "cv_scores_log = cross_validate(log_reg, X_scaled, y, scoring=('accuracy', 'precision', 'recall', 'roc_auc'), cv=10, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average score both train and test sets\n",
    "avg_train_accuracy = (np.mean(cv_scores_log[\"train_accuracy\"]))\n",
    "avg_test_accuracy = (np.mean(cv_scores_log[\"test_accuracy\"]))\n",
    "\n",
    "avg_train_precision = (np.mean(cv_scores_log[\"train_precision\"]))\n",
    "avg_test_precision = (np.mean(cv_scores_log[\"test_precision\"]))\n",
    "\n",
    "avg_train_recall = (np.mean(cv_scores_log[\"train_recall\"]))\n",
    "avg_test_recall = (np.mean(cv_scores_log[\"test_recall\"]))\n",
    "\n",
    "avg_train_auc = (np.mean(cv_scores_log[\"train_roc_auc\"]))\n",
    "avg_test_auc = (np.mean(cv_scores_log[\"test_roc_auc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV accuracy scores: \n",
      "\tTrain set: 0.9882835343567251\n",
      "\tTest set: 0.9806704260651629\n",
      "\n",
      "Average CV precision scores: \n",
      "\tTrain set: 0.9861280527816707\n",
      "\tTest set: 0.9784455508139718\n",
      "\n",
      "Average CV recall scores: \n",
      "\tTrain set:0.995330005224357\n",
      "\tTest set: 0.9915079365079364\n",
      "\n",
      "Average CV ROC AUC scores: \n",
      "\tTrain set: 0.9975613439407678\n",
      "\tTest set:0.9961722668865525\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "# Accuracy\n",
    "print(\"Average CV accuracy scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_accuracy}\")\n",
    "print(f\"\\tTest set: {avg_test_accuracy}\")\n",
    "\n",
    "# Precision\n",
    "print(\"\\nAverage CV precision scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_precision}\")\n",
    "print(f\"\\tTest set: {avg_test_precision}\")\n",
    "\n",
    "# Recall\n",
    "print(\"\\nAverage CV recall scores: \")\n",
    "print(f\"\\tTrain set:{avg_train_recall}\")\n",
    "print(f\"\\tTest set: {avg_test_recall}\")\n",
    "\n",
    "# Roc Auc\n",
    "print(\"\\nAverage CV ROC AUC scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_auc}\")\n",
    "print(f\"\\tTest set:{avg_test_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "\n",
    "- Training scores and test scores are pretty much the same across all scores displayed. Because of that, it seems the model performs very well given the high scores across all the metrics displayed. \n",
    "- Since we want to know whether a tumor is malignant or benign, it would be ideal for this model to have recall score as a metric for evaluation (it is preferrable to misclassify a benign tumor as malign than a malign tumor as benign, thus there is a higher costs in false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2 (10 points) \n",
    "By default, sklearn's logistic regression uses the L2 regularization. Now use the logistic regression without any regularzation to perform cross validation, report what do you find on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with no regularization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_scaled = StandardScaler().fit_transform(X) \n",
    "log_reg_none = LogisticRegression(penalty = 'none', max_iter = 5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation \n",
    "cv_scores_log_none = cross_validate(log_reg_none, X_scaled, y, scoring=('accuracy', 'precision', 'recall', 'roc_auc'), cv=10, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average score both train and test sets\n",
    "avg_train_accuracy = (np.mean(cv_scores_log_none[\"train_accuracy\"]))\n",
    "avg_test_accuracy = (np.mean(cv_scores_log_none[\"test_accuracy\"]))\n",
    "\n",
    "avg_train_precision = (np.mean(cv_scores_log_none[\"train_precision\"]))\n",
    "avg_test_precision = (np.mean(cv_scores_log_none[\"test_precision\"]))\n",
    "\n",
    "avg_train_recall = (np.mean(cv_scores_log_none[\"train_recall\"]))\n",
    "avg_test_recall = (np.mean(cv_scores_log_none[\"test_recall\"]))\n",
    "\n",
    "avg_train_auc = (np.mean(cv_scores_log_none[\"train_roc_auc\"]))\n",
    "avg_test_auc = (np.mean(cv_scores_log_none[\"test_roc_auc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV accuracy scores: \n",
      "\tTrain set: 1.0\n",
      "\tTest set: 0.9490601503759398\n",
      "\n",
      "Average CV precision scores: \n",
      "\tTrain set: 1.0\n",
      "\tTest set: 0.9723910935985239\n",
      "\n",
      "Average CV recall scores: \n",
      "\tTrain set:1.0\n",
      "\tTest set: 0.9466666666666667\n",
      "\n",
      "Average CV ROC AUC scores: \n",
      "\tTrain set: 1.0\n",
      "\tTest set:0.9875898440184155\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "# Accuracy\n",
    "print(\"Average CV accuracy scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_accuracy}\")\n",
    "print(f\"\\tTest set: {avg_test_accuracy}\")\n",
    "\n",
    "# Precision\n",
    "print(\"\\nAverage CV precision scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_precision}\")\n",
    "print(f\"\\tTest set: {avg_test_precision}\")\n",
    "\n",
    "# Recall\n",
    "print(\"\\nAverage CV recall scores: \")\n",
    "print(f\"\\tTrain set:{avg_train_recall}\")\n",
    "print(f\"\\tTest set: {avg_test_recall}\")\n",
    "\n",
    "# Roc Auc\n",
    "print(\"\\nAverage CV ROC AUC scores: \")\n",
    "print(f\"\\tTrain set: {avg_train_auc}\")\n",
    "print(f\"\\tTest set:{avg_test_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "\n",
    "- Without regularization in the model, we see that all the test scores are lower (although not by much)\n",
    "- Train scores are 1, possibly due to lack of regularization, the algorithm tries to reduce as much error as possible and without regularization the coefficient is not shrinked.\n",
    "\n",
    "##### Comment:\n",
    "\n",
    "- All test scores that we are displaying are lower (precision by a tiny bit). Without L2 regularization, the logistic regression coefficients do not have the added penalty that the L2 would add to them depending on the alpha/Lambda value. Regularization in Logistic Regression is important because of the logistic regression's asymptotic nature. Naturally, the regression would keep driving loss towards 0 in high dimensions. That's why L2 regularization is used, it softens the model complexity \n",
    "- The previous point occurs because if we don't have a regularization function, the logistic model would drive loss to zero on all instances and the weights for each indicator feature to +infinity or -infinity. \n",
    "- More specifically, in some cases the model wants to predict only P=1 or P=0 (Perfect separation). When this occcurs, the model attempts to predict as close to 0 and 1 as possible. To achieve this, it must set the regression weights as large as possible. Regularization helps overcome this issue because it avoids the model setting the regression weights infinitely large.\n",
    "- I decided to use scaled features because it optimized a little bit the performance and converged faster\n",
    "\n",
    "Reference: https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.3 (10 points) \n",
    "Check how many Benign and Malignant cases in the dataset. What might be the problem if we use the default score of the logistic regression cross-validation? Now adjust the class weight of M and L and retrain the model again to bias toward Malignant, using the relative weight of M and L as 2:1. What about the relaive weight to be 5:1, or 10:1? Explain what you find.\n",
    "\n",
    "Hint: you can use LogisticRegressionCV to combine LogisticRegression and cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 212 malignant cases, and a total of 357 benign cases, which corresponds to 37.26 % for malignant cases and to 62.74 % for benign cases\n"
     ]
    }
   ],
   "source": [
    "# Count benign and malignant cases\n",
    "df_y = pd.DataFrame(y)\n",
    "normalized = df_y.value_counts(normalize=True)\n",
    "total_mal = np.count_nonzero(y == 0)\n",
    "total_ben = np.count_nonzero(y == 1)\n",
    "\n",
    "print(f\"There are a total of {total_mal} malignant cases, and a total of {total_ben} benign cases, which corresponds to {np.round(normalized[0]*100, 2)} % for malignant cases and to {np.round(normalized[1]*100,2)} % for benign cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What might be the problem if we use the default score of the logistic regression cross-validation?\n",
    "\n",
    "- The problem is that we have an unbalanced dataset\n",
    "- Unbalanced classes in our dataset create challenges during the prediction because of the skewed class distribution. More specifically, it might cause poor performance with evaluation metrics that assume a balanced class distribution.\n",
    "- In the case of the logistic regression, the default metric is accuracy, which equals the total correct predictions over the total number of instances. Our predictions may be biased towards the class that there is more data points of -- 1. there is more information on the majority class and 2. having a high accuracy does not equal to a good model as by predicting all instances for the majority class would already give a relatively high accuracy score. As a result, the metric of accuracy kind of loses the common-meaning of \"accurate model\" (unless we know the formulas and the class balances).\n",
    "- In this case, by predicting all instances to be benign we would already have a 63 % accuracy - not great but already more than ranodm guessing.\n",
    "- Overall, unbalanced datasets give us biased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjusting the class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot confusion matrix -- helpful for visualization\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(matrix):\n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# TN, FP \n",
    "# FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.85      0.89       212\n",
      "      benign       0.92      0.96      0.94       357\n",
      "\n",
      "    accuracy                           0.92       569\n",
      "   macro avg       0.93      0.91      0.92       569\n",
      "weighted avg       0.92      0.92      0.92       569\n",
      "\n",
      "ROC AUC score is 0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original weights -- just to compare\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegressionCV(cv=10, random_state=42, max_iter=5000, solver=\"saga\").fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(classification_report(y, y_pred, target_names= dataset[\"target_names\"]))\n",
    "\n",
    "print(f\"ROC AUC score is {np.round(roc_auc_score(y, y_pred), 2)}\")\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.90      0.89      0.89       212\n",
      "      benign       0.93      0.94      0.94       357\n",
      "\n",
      "    accuracy                           0.92       569\n",
      "   macro avg       0.91      0.91      0.91       569\n",
      "weighted avg       0.92      0.92      0.92       569\n",
      "\n",
      "ROC AUC score is 0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adjust the class weight of M and L and retrain the model again to bias toward Malignant\n",
    "# using the relative weight of M and L as 2:1\n",
    "\n",
    "# Weight M:L 2:1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegressionCV(cv=10, random_state=42, solver=\"saga\", max_iter= 5000, class_weight={0:2, 1:1}).fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(classification_report(y, y_pred, target_names= dataset[\"target_names\"]))\n",
    "\n",
    "# Let's also print Roc Auc scores (to check these shouldn't change as weights change)\n",
    "print(f\"ROC AUC score is {np.round(roc_auc_score(y, y_pred), 2)}\")\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.80      0.95      0.87       212\n",
      "      benign       0.97      0.86      0.91       357\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.88      0.90      0.89       569\n",
      "weighted avg       0.90      0.89      0.89       569\n",
      "\n",
      "ROC AUC score is 0.9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weight M:L 5:1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegressionCV(cv=10, random_state=42, solver=\"saga\", max_iter= 5000, class_weight={0:5, 1:1}).fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(classification_report(y, y_pred, target_names= dataset[\"target_names\"]))\n",
    "\n",
    "print(f\"ROC AUC score is {np.round(roc_auc_score(y, y_pred), 2)}\")\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.73      0.98      0.83       212\n",
      "      benign       0.98      0.78      0.87       357\n",
      "\n",
      "    accuracy                           0.85       569\n",
      "   macro avg       0.85      0.88      0.85       569\n",
      "weighted avg       0.89      0.85      0.86       569\n",
      "\n",
      "ROC AUC score is 0.88\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weight M:L 10:1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegressionCV(cv=10, random_state=42, solver=\"saga\", max_iter=5000, class_weight={0:10, 1:1}).fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(classification_report(y, y_pred, target_names= dataset[\"target_names\"]))\n",
    "\n",
    "print(f\"ROC AUC score is {np.round(roc_auc_score(y, y_pred), 2)}\")\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "- As expected, ROC AUC scores remained the same despite the class imbalance, confirming that ROC is insensitive to class distribution/unbalanced datasets.\n",
    "- For the distribution of 2:1 M:L, accuracy was the same that with the original weights. This is because the original weights were similar to 1:2 M:L (almost 400 cases vs 200)\n",
    "- As we increased our biased towards malignant cases, the accuracy decreased as well. This is expected since as mentioned before accuracy does not perform well on unbalanced datasets.\n",
    "- Regarding both precision and accuracy macro averages, they also decreased as the dataset became more unbalanced. \n",
    "- When looking deeper into the metrics, the recall for malignant cases increased as the weights for this class became higher. On the contrary, the precision for this class decreased. \n",
    "- Overall, there was an increase in predicting benign cases as malignant (false negatives)\n",
    "\n",
    "Comment:\n",
    "\n",
    "- The reason why a bias towards maligant cases generated a better recall for this class and less precision is because recall considers only true positives and false negatives (actual positives). So, within a dataset heavily weighted to malignant values (those that are 0 in this case), the model would bias its estimates towards these values by increasing the number of false negatives predictions (there are more and more benign cases predicted as malignant, and very small number of malignant predicted as benign)\n",
    "- The same logic applies with precision. If our model is biased towards malignant cases, there are probably going to be less false positives (less malignant cases predicted as benign)\n",
    "- As we have studied, there is a tradeoff between precision and recall.\n",
    "- Changing the solver type helped converge, otherwise an error stated that max iterations were reached for the lbgfs solver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
